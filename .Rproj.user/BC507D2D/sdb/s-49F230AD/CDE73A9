{
    "collab_server" : "",
    "contents" : "# we need to adjust the output for the protein and Genomesequence methods\n#\nrequire(BiocParallel)\nfetchCellbase <- function(file=NULL,host=host, version=version, meta=meta,species=species, categ, subcateg,ids,resource,filters=NULL,batch_size=NULL,num_threads=NULL,...){\n  batch_size <- batch_size\n  num_threads <- num_threads\n  if(is.null(categ)){\n    categ <- \"\"\n  }else{\n    categ <- paste0(categ,\"/\",sep=\"\")\n  }\n  if(is.null(subcateg)){\n    subcateg <- \"\"\n  }else{\n    subcateg <- paste0(subcateg,\"/\",sep=\"\")\n  }\n  if(is.null(file)){\n    if(is.null(ids)){\n      ids <- \"\"\n    }else{\n      ids <- paste0(ids,collapse = \",\")\n      ids <- paste0(ids,\"/\",collapse = \"\")\n    }\n\n  }else{\n    ids <- readIds(file,batch_size = batch_size,num_threads = num_threads)\n  }\n\n  if(!is.null(file)){\n    container=list()\n    grls <- createURL(file = file,host=host,version=version,species=species,categ=categ,subcateg=subcateg,ids=ids,resource=resource,...)\n    content <- callREST(grls = grls,async=TRUE,num_threads)\n    res_list <- parseResponse(content=content,parallel=TRUE, num_threads=num_threads)\n    ds <- res_list$result\n\n\n  }else{\n    i=1\n    server_limit=1000\n    skip=0\n    num_results=1000\n    container=list()\n    while(is.null(file)&all(unlist(num_results)==server_limit)){\n      grls <- createURL(file=NULL,host=host, version=version, meta=meta, species=species, categ=categ,subcateg=subcateg,ids=ids,resource=resource,filters=filters,skip = skip)\n      skip=skip+1000\n      content <- callREST(grls = grls)\n      res_list <- parseResponse(content=content)\n      num_results <- res_list$num_results\n      cell <- res_list$result\n      container[[i]] <- cell\n      i=i+1\n    }\n    ds <- rbind.pages(container)\n  }\n\n\n  return(ds)\n}\n\nreadIds <- function(file=file,batch_size,num_threads)\n  {\n  require(Rsamtools)\n  #require(pbapply)\n  ids<- list()\n  num_iter<- ceiling(R.utils::countLines(file)[[1]]/(batch_size*num_threads))\n  #batchSize * numThreads\n  demo <- TabixFile(file,yieldSize = batch_size*num_threads)\n  tbx <- open(demo)\n  i <- 1\n  while (i <=num_iter) {\n    inter <- scanTabix(tbx)[[1]]\n    if(length(inter)==0)break\n    whim <- lapply(inter, function(x){strsplit(x[1],split = \"\\t\")[[1]][c(1,2,4,5)]})\n    whish <- sapply(whim, function(x){paste(x,collapse =\":\")})\n    hope <- split(whish, ceiling(seq_along(whish)/batch_size))\n    ids[[i]] <- hope\n    i <- i+1\n  }\n  ids <- pbsapply(ids, function(x)lapply(x, function(x)x))\n  return(ids)\n}\n\n  #create a list of character vectors of urls\ncreateURL <- function(file=NULL,host=host,version=version,meta=meta,species=species,categ=categ,subcateg=subcateg,ids=ids,resource=resource,filters=filters,skip=0)\n  {\n\n  if(is.null(file)){\n    skip=paste0(\"?\",\"skip=\",skip)\n    filters <- paste(skip,filters,sep = \"&\")\n    grls <- paste0(host,version,meta,species,categ,subcateg,ids,resource,filters,collapse = \"\")\n\n  }else{\n    grls <- list()\n    gcl <- paste0(host,version,species,categ,subcateg,collapse = \"\")\n\n    for(i in seq_along(ids)){\n      hop <- paste(ids[[i]],collapse = \",\")\n      tmp <- paste0(gcl,hop,resource,collapse = \",\")\n      grls[[i]] <- gsub(\"chr\",\"\",tmp)\n    }\n  }\n  return(grls)\n}\ncallREST <- function(grls,async=FALSE,num_threads=num_threads){\n  content <- list()\n\n  require(RCurl)\n  if(is.null(file)){\n    content <- getURI(grls)\n  }else{\n    require(pbapply)\n    if(async==TRUE){\n      prp <- split(grls,ceiling(seq_along(grls)/num_threads))\n      cat(\"Preparing The Asynchronus call.............\")\n      gs <- pblapply(prp, function(x)unlist(x))\n      cat(\"Getting the Data...............\")\n      content <- pblapply(gs,function(x)getURIAsynchronous(x,perform = Inf))\n      content <- unlist(content)\n\n    }else{\n      content <- pbsapply(grls, function(x)getURI(x))\n\n    }\n  }\n\n\n  return(content)\n}\nparseResponse <- function(content,parallel=FALSE,num_threads=num_threads){\n\n  require(jsonlite)\n  if(parallel==TRUE){\n    # require(parallel)\n    # require(doMC)\n    # num_cores <-detectCores()/2\n    # registerDoMC(num_cores)\n    \n    ### Extracting the content in parallel\n    # js <- mclapply(content, function(x)fromJSON(x),mc.cores=num_cores)\n    # res <- mclapply(js, function(x)x$response$result,mc.cores=num_cores)\n    # ds <- mclapply(res, function(x)rbind.pages(x),mc.cores=num_cores)\n    js <- bplapply(content, function(x)fromJSON(x))\n    res <- pblapply(js, function(x)x$response$result)\n    ds <- pblapply(res, function(x)rbind.pages(x))\n    ### Important to get correct merging of dataframe\n    names(ds) <- NULL\n    ds <- rbind.pages(ds)\n    nums <- NULL\n    # js <- lapply(content, function(x)fromJSON(x))\n    # ares <- lapply(js, function(x)x$response$result)\n    # ds <- pblapply(ares,function(x)rbind.pages(x))\n  }else{\n  js <- lapply(content, function(x)fromJSON(x))\n  ares <- lapply(js, function(x)x$response$result)\n  nums <- lapply(js, function(x)x$response$numResults)\n  ds <- pblapply(ares,function(x)rbind.pages(x))\n  ### Important to get correct vertical binding of dataframes\n  names(ds) <- NULL\n  ds <- rbind.pages(ds)\n  }\n  return(list(result=ds,num_results=nums))\n}\n",
    "created" : 1461019162880.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1086179802",
    "id" : "CDE73A9",
    "lastKnownWriteTime" : 1461190155,
    "last_content_update" : 1461190155,
    "path" : "~/R/cellbase/clients/R/R/cellbase_functions.R",
    "project_path" : "R/cellbase_functions.R",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}